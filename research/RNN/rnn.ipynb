{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Readability Assessment through Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Readability assessment is a well known problem in natural language processing field. Giving someone the suitable text for his level of comprehension (not so easy and not so hard) could maximize his understanding and enjoyment. In this notebook we are trying to assess the readability of a given text regardless of the text subject using recurrent neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Corpus\n",
    "> OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification  \n",
    "> Sowmya Vajjala and Ivana Lučić  \n",
    "> 2018  \n",
    "> Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 297–304. Association for Computational Linguistics.  \n",
    "> [url](http://aclweb.org/anthology/W18-0535). [bib file](https://aclanthology.coli.uni-saarland.de/papers/W18-0535/w18-0535.bib)\n",
    "\n",
    "Please cite the above paper if you use this corpus in your research.\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/128919409.svg)](https://zenodo.org/badge/latestdoi/128919409)\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's dive into our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.path.append(\"/home/ms10596/Documents/match\")\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from research.utils.one_stop_english import OneStopEnglish\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML\n",
    "from research.utils.loading import load_glove_embeddings\n",
    "corpus = OneStopEnglish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading level|Avg. Num. Words|Std. Dev|Number of Articles\n",
    "---|---|---|---\n",
    "Elementary|533.17|103.79|189\n",
    "Intermediate|676.59|117.15|189\n",
    "Advanced|820.49|162.52|189\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0992278be684ee58d7d47aabd44183c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=94, description='i', max=188), IntSlider(value=500, description='words',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def show_articles(i=(0,188,1), words=(0,1000,1)):\n",
    "    data = [\n",
    "        [\"Advanced\",corpus.articles[i+189+189][:words]], \n",
    "        [\"Intermediate\",corpus.articles[i+189][:words]], \n",
    "        [\"Elementary\",corpus.articles[i][:words]]\n",
    "    ]\n",
    "    headers = ['Reading Level', 'Example']\n",
    "    display(HTML(tabulate(data,tablefmt='html', headers=headers)+\"<style>th,td {font-size: 10px}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~Feature Extraction~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7738e8a859c442a5bfa1ee827e0fc980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=30, description='i', max=60), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def show_features(i=(0,60,1)):\n",
    "    data = [[\"Adjectives (ADJ) like warm, fat\"],[\"Adverbs (ADV) like almost, too, very\"],[\"Articles (ART) like a, an, the\"],[\"Conjunctions (CONJ) like for, and, nor\"],[\"Interjections (INTERJ) like wow, oops, ouch\"],[\"Nouns (NOUN) like boy, girl, doctor, town\"],[\"Numerals (NUM) like 1, 155, 89\"],[\"Past participles (PASTPART) like taken, eaten\"],[\"Prepositions (PREP) like at, for, in, off\"],[\"Pronouns (PRON) like he, she, you, I\"],[\"Punctuation (PUNCT) like ?, :, ;, .\"],[\"Special symbols (SYMBOL) like @, %,\"] ,[\"Adjectives (ADJ) like warm, fat\"],[\"Adverbs (ADV) like almost, too, very\"],[\"Articles (ART) like a, an, the\"],[\"Conjunctions (CONJ) like for, and, nor\"],[\"Interjections (INTERJ) like wow, oops, ouch\"],[\"Nouns (NOUN) like boy, girl, doctor, town\"],[\"Numerals (NUM) like 1, 155, 89\"],[\"Past participles (PASTPART) like taken, eaten\"],[\"Prepositions (PREP) like at, for, in, off\"],[\"Pronouns (PRON) like he, she, you, I\"],[\"Punctuation (PUNCT) like ?, :, ;, .\"],[\"Special symbols (SYMBOL) like @, %, \"],[\"Nominal phrases (NP) like The dog on the sofa\"],[\"Adjectival phrases (AP) like The movie was terrible\"],[\"Prepositional phrases (PP) like We stayed by the river\"],[\"Adverbial phrases (ADVP) like The carpenter hit the nail with a hammer.\"],[\"Temporal auxiliary verb phrases (VTEMP) like I was wondering about.\"],[\"Aspectual auxiliary verb phrases (VASP) l have seen the light\"],[\"Modal auxiliary verb phrases (VMOD) I should study\"],[\"Copulative verb phrases (VCOP) John is happy\"],[\"Past participle verb phrases (VPASTPART) I have bought\"],[\"Gerundive verb phrases (VGER) Running is a good exercise\"],[\"Infinitive verb phrases (VINF) I want to study.\"],[\"Finite verb phrases (VF) She plays guitar\"],[\"Sub-clause phrases (SC e REL)Until Mr. Sanchez has his first cup\"],[\"Verb phrases (VF e VCOP) Ali is going to school\"],[\"Number of sentences\"],[\"Number of words\"],[\"Number of different words\"],[\"Number of different verbs forms\"],[\"Number of auxiliary verbs\"],[\"Number of main verbs\"],[\"Average number of verb phrases per sentence\"],[\"Average length of sentences\"],[\"Average length of syllables per word\"],[\"Average size of verbal chains\"],[\"Average size of coordination relation’s chains\"],[\"Frequency of words with 1-4 syllables\"],[\"Frequency of words with more than 4 syllables\"],[\"Total number of dependencies\"],[\"Total number of tree nodes\"],[\"Number of pronouns per noun phrases (NP)\"],[\"Number of NP with a definite or demonstrative determiner\"],[\"Number of NP with a indefinite determiner\"],[\"Number of subordinate clauses (SC/REL chunks)\"], [\"Number of coordination relations\"], [\"Number of omit subjects\"],[\"Flesch Reading Ease BR readability measure\"]]\n",
    "    display(HTML(tabulate(data[0:i],tablefmt='html')+\"<style>th,td {font-size: 10px}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we see there are plenty of features we can extract.\n",
    "* Feature Engineering is a tedious process and needs a great experience.\n",
    "* Also those features neglect sentence structure as they destroy the ordering of text.\n",
    "> so we are not going to extract features. every word will be feature itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
