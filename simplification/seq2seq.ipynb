{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ms10596/PycharmProjects/match\")\n",
    "from utils.one_stop_english import load_advanced_elementary\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from statistics import mean\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced, elementary = load_advanced_elementary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean len of adv: 126.8915050784857\n",
      "mean len of elem: 111.70452446906741\n"
     ]
    }
   ],
   "source": [
    "print(\"mean len of adv:\", mean([len(i) for i in advanced]))\n",
    "print(\"mean len of elem:\", mean([len(i) for i in elementary]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "china is the biggest coal importer, and indonesia the biggest exporter, having temporarily overtaken australia.\n",
      "china is the biggest coal importer, and indonesia is the biggest coal exporter.\n"
     ]
    }
   ],
   "source": [
    "print(advanced[50])\n",
    "print(elementary[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2166,)\n",
      "[1, 3290, 475, 197, 22, 1089, 9, 34, 948, 4, 28, 5, 118, 233, 715, 412, 1430, 517, 17, 1, 164, 302, 1876, 1090, 41, 64, 1297, 1, 96, 3, 41, 277, 829, 9, 884, 1877, 1, 3291, 3, 1091, 1878, 2, 71, 204, 885, 606]\n",
      "the seattle-based company has applied for its brand to be a top-level domain name (currently .com), but the south american governments argue this would prevent the use of this internet address for environmental protection, the promotion of indigenous rights and other public interest uses.\n"
     ]
    }
   ],
   "source": [
    "source_sequences = tokenizer.texts_to_sequences(advanced)\n",
    "print(np.shape(source_sequences))\n",
    "print(source_sequences[0])\n",
    "print(advanced[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2166,)\n",
      "[887, 22, 350, 9, 34, 197, 412, 4, 28, 5, 118, 233, 715, 412, 1430, 517, 17, 1, 164, 302, 1876, 212, 41, 64, 652, 1, 96, 3, 41, 277, 829, 9, 884, 1877, 1091, 1878, 2, 71, 204, 885, 606]\n",
      "amazon has asked for its company name to be a top-level domain name (currently .com), but the south american governments say this would stop the use of this internet address for environmental protection, indigenous rights and other public interest uses.\n"
     ]
    }
   ],
   "source": [
    "target_sequences = tokenizer.texts_to_sequences(elementary)\n",
    "print(np.shape(target_sequences))\n",
    "print(target_sequences[0])\n",
    "print(elementary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean len of adv: 60\n",
      "mean len of elem: 52\n"
     ]
    }
   ],
   "source": [
    "print(\"mean len of adv:\", max([len(i) for i in source_sequences]))\n",
    "print(\"mean len of elem:\", max([len(i) for i in target_sequences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2166, 60, 5761)\n",
      "(2166, 60)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "padded_target_sequences = pad_sequences(target_sequences, maxlen=60, padding='post', truncating='post')\n",
    "padded_source_sequences = pad_sequences(source_sequences, maxlen=60, padding='post', truncating='post')\n",
    "padded_target_sequences = to_categorical(padded_target_sequences)\n",
    "print(np.shape(padded_target_sequences))\n",
    "print(np.shape(padded_source_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ms10596/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1841 samples, validate on 325 samples\n",
      "Epoch 1/500\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, RepeatVector, Dense\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow import keras\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss',patience=10, verbose=0, mode='auto')\n",
    "\n",
    "n_units = 256\n",
    "model = Sequential()\n",
    "model.add(Embedding(5761, n_units, input_length=60, mask_zero=True))\n",
    "# model.add(LSTM(n_units))\n",
    "# model.add(RepeatVector(tar_timesteps))\n",
    "model.add(LSTM(n_units, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(5761, activation='softmax')))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit(padded_source_sequences, padded_target_sequences, epochs=500, batch_size=64, verbose=2, validation_split=0.15,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {i:word for word,i in tokenizer.word_index.items()}\n",
    "id_to_word[0] = \"<space>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.predict(padded_source_sequences[50:51])[0]:\n",
    "#     print(np.shape(i))\n",
    "    val = np.argmax(i)\n",
    "    if val < 5761:\n",
    "        print(id_to_word[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sha2y.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
