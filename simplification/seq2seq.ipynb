{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ms10596/PycharmProjects/match\")\n",
    "from utils.one_stop_english import load_advanced_elementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced, elementary = load_advanced_elementary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean len of adv: 126.8915050784857\n",
      "mean len of elem: 111.70452446906741\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(\"mean len of adv:\", mean([len(i) for i in advanced]))\n",
    "print(\"mean len of elem:\", mean([len(i) for i in elementary]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5760"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(advanced+elementary)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2166,)\n",
      "[1, 3290, 475, 197, 22, 1089, 9, 34, 948, 4, 28, 5, 118, 233, 715, 412, 1430, 517, 17, 1, 164, 302, 1876, 1090, 41, 64, 1297, 1, 96, 3, 41, 277, 829, 9, 884, 1877, 1, 3291, 3, 1091, 1878, 2, 71, 204, 885, 606]\n",
      "the seattle-based company has applied for its brand to be a top-level domain name (currently .com), but the south american governments argue this would prevent the use of this internet address for environmental protection, the promotion of indigenous rights and other public interest uses.\n"
     ]
    }
   ],
   "source": [
    "source_sequences = tokenizer.texts_to_sequences(advanced)\n",
    "import numpy as np\n",
    "print(np.shape(source_sequences))\n",
    "print(source_sequences[0])\n",
    "print(advanced[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2166,)\n",
      "[887, 22, 350, 9, 34, 197, 412, 4, 28, 5, 118, 233, 715, 412, 1430, 517, 17, 1, 164, 302, 1876, 212, 41, 64, 652, 1, 96, 3, 41, 277, 829, 9, 884, 1877, 1091, 1878, 2, 71, 204, 885, 606]\n",
      "amazon has asked for its company name to be a top-level domain name (currently .com), but the south american governments say this would stop the use of this internet address for environmental protection, indigenous rights and other public interest uses.\n"
     ]
    }
   ],
   "source": [
    "target_sequences = tokenizer.texts_to_sequences(elementary)\n",
    "print(np.shape(target_sequences))\n",
    "print(target_sequences[0])\n",
    "print(elementary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean len of adv: 60\n",
      "mean len of elem: 52\n"
     ]
    }
   ],
   "source": [
    "print(\"mean len of adv:\", max([len(i) for i in source_sequences]))\n",
    "print(\"mean len of elem:\", max([len(i) for i in target_sequences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2166, 60, 5761)\n",
      "(2166, 60)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "padded_target_sequences = pad_sequences(target_sequences, maxlen=60, padding='post', truncating='post')\n",
    "padded_source_sequences = pad_sequences(source_sequences, maxlen=60, padding='post', truncating='post')\n",
    "padded_target_sequences = to_categorical(padded_target_sequences)\n",
    "print(np.shape(padded_target_sequences))\n",
    "print(np.shape(padded_source_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dense' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8c5c36641d3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# model.add(RepeatVector(tar_timesteps))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5761\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_source_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_target_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dense' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.embeddings import Embedding\n",
    "n_units = 256\n",
    "model = Sequential()\n",
    "model.add(Embedding(5761, n_units, input_length=60, mask_zero=True))\n",
    "# model.add(LSTM(n_units))\n",
    "# model.add(RepeatVector(tar_timesteps))\n",
    "model.add(LSTM(n_units, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(5761, activation='softmax')))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit(padded_source_sequences, padded_target_sequences, epochs=100, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {i:word for word,i in tokenizer.word_index.items()}\n",
    "id_to_word[0] = \"<space>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.predict(padded_source_sequences[50:51])[0]:\n",
    "#     print(np.shape(i))\n",
    "    val = np.argmax(i)\n",
    "    if val < 5761:\n",
    "        print(id_to_word[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
