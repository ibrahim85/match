{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Readability Assessment through Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Readability assessment is a well known problem in natural language processing field. \n",
    "* Giving someone the suitable text for his level of comprehension (not so easy and not so hard) could maximize his understanding and enjoyment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this notebook we are trying to assess the readability of a given text regardless of the text topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Corpus\n",
    "> OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification  \n",
    "> Sowmya Vajjala and Ivana Lučić  \n",
    "> 2018  \n",
    "> Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 297–304. Association for Computational Linguistics.  \n",
    "> [url](http://aclweb.org/anthology/W18-0535). [bib file](https://aclanthology.coli.uni-saarland.de/papers/W18-0535/w18-0535.bib)\n",
    "\n",
    "Please cite the above paper if you use this corpus in your research.\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/128919409.svg)](https://zenodo.org/badge/latestdoi/128919409)\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's dive into our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ms10596/PycharmProjects/match\")\n",
    "from ipywidgets import interact\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from utils.loading import load_glove_embeddings, load_old_corpus\n",
    "from utils.one_stop_english import load_corpus, corpus_to_words, corpus_to_pos, detokenize\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, LSTM, Bidirectional,Conv1D,MaxPooling1D,GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "corpus = load_corpus()\n",
    "articles_words, tags = corpus_to_words(corpus)\n",
    "articles_pos, tags = corpus_to_pos(corpus)\n",
    "old_articles_pos, old_articles_tags = load_old_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reading level|Avg. Num. Words|Std. Dev|Number of Articles\n",
    "---|---|---|---\n",
    "Elementary|533.17|103.79|189\n",
    "Intermediate|676.59|117.15|189\n",
    "Advanced|820.49|162.52|189\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b30d94969e445dcbca9fe974100704a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=94, description='i', max=188), IntSlider(value=500, description='words',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def show_articles(i=(0,188,1), words=(0,1000,1)):\n",
    "    data = [\n",
    "        [\"Advanced\",detokenize(articles_words[i][:words])], \n",
    "        [\"Intermediate\",detokenize(articles_words[i+2][:words])], \n",
    "        [\"Elementary\",detokenize(articles_words[i+1][:words])]\n",
    "    ]\n",
    "    headers = ['Reading Level', 'Example']\n",
    "    display(HTML(tabulate(data,tablefmt='html', headers=headers)+\"<style>th,td {font-size: 20px}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'you', 'see', 'the', 'word', 'Amazon', ',', 'whats', 'the', 'first']\n",
      "['WRB', 'PRP', 'VB', 'DT', 'NN', 'NN', ',', 'VBZ', 'DT', 'JJ']\n"
     ]
    }
   ],
   "source": [
    "print(articles_words[0][:10])\n",
    "print(articles_pos[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nn': 1, 'in': 2, 'dt': 3, 'jj': 4, 'nns': 5, 'nnp': 6, ',': 7, '.': 8, 'rb': 9, 'prp': 10, 'vb': 11, 'vbd': 12, 'cc': 13, 'vbz': 14, 'to': 15, 'vbp': 16, 'cd': 17, 'vbn': 18, 'vbg': 19, 'prp$': 20, 'md': 21, 'wdt': 22, 'wrb': 23, 'wp': 24, 'jjr': 25, 'rp': 26, ':': 27, 'jjs': 28, 'ex': 29, 'rbr': 30, 'nnps': 31, '-rrb-': 32, '-lrb-': 33, 'rbs': 34, 'pdt': 35, '$': 36, 'fw': 37, 'uh': 38, 'wp$': 39, 'sym': 40, \"''\": 41, 'ls': 42, 'pos': 43, '``': 44}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "maxlen = 1000 # Cuts off reviews after 1000 words\n",
    "max_words = 45\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(articles_pos)\n",
    "\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer.to_json(), ensure_ascii=False))\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567,)\n",
      "(183,)\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(articles_pos)\n",
    "old_sequences = tokenizer.texts_to_sequences(old_articles_pos)\n",
    "print(np.shape(sequences))\n",
    "print(np.shape(old_sequences))\n",
    "# print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567, 1000)\n",
      "(183, 1000)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "old_data = pad_sequences(old_sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "# print(data[0])\n",
    "print(np.shape(data))\n",
    "print(np.shape(old_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "(567,)\n",
      "(183,)\n"
     ]
    }
   ],
   "source": [
    "print(tags[0])\n",
    "print(old_articles_tags[0])\n",
    "tags = np.array(tags)\n",
    "old_tags = np.array(old_articles_tags)\n",
    "print(tags.shape)\n",
    "print(old_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567, 1000, 45)\n",
      "(183, 1000, 45)\n"
     ]
    }
   ],
   "source": [
    "data = to_categorical(data)\n",
    "old_data = to_categorical(old_data)\n",
    "print(data.shape)\n",
    "print(old_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567, 1000, 45)\n",
      "(567,)\n",
      "(183, 1000, 45)\n",
      "(183,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "data = data[indices]\n",
    "tags = tags[indices]\n",
    "\n",
    "old_indices = np.arange(old_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "old_data = old_data[old_indices]\n",
    "old_tags = old_tags[old_indices]\n",
    "print(data.shape)\n",
    "print(tags.shape)\n",
    "\n",
    "print(old_data.shape)\n",
    "print(old_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(128, 5, activation='relu',kernel_regularizer=l1_l2(l1=0.01, l2=0.01), input_shape=(1000,45)))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(64, 5, activation='relu',kernel_regularizer=l1_l2(l1=0.001, l2=0.001)))\n",
    "# model.add(MaxPooling1D(5))\n",
    "# model.add(Conv1D(32, 5, activation='relu',kernel_regularizer=l1_l2(l1=0.0003, l2=0.0003)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1))\n",
    "# model.summary()\n",
    "def soft_acc(y_true, y_pred):\n",
    "    from tensorflow.python.keras import backend as K\n",
    "    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.40350887649938\n",
      "66.66666792150129\n",
      "49.122808272378485\n",
      "45.614034617156314\n",
      "64.91228080632393\n",
      "54.385964807711154\n",
      "57.89473715581393\n",
      "73.68421178115042\n",
      "64.91228101546305\n",
      "55.555556217829384\n",
      "Accuracy: 59.415205147182725 std: 8.120868247309904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "cvs_scores = []\n",
    "for train, test in kfold.split(data, tags):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(128, 5, activation='relu',kernel_regularizer=l1_l2(l1=0.01, l2=0.01), input_shape=(1000,45)))\n",
    "    model.add(MaxPooling1D(5))\n",
    "    model.add(Conv1D(64, 5, activation='relu',kernel_regularizer=l1_l2(l1=0.001, l2=0.001)))\n",
    "    model.add(MaxPooling1D(5))\n",
    "    model.add(Conv1D(32, 5, activation='relu',kernel_regularizer=l1_l2(l1=0.0003, l2=0.0003)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(1))\n",
    "    # model.summary()\n",
    "    def soft_acc(y_true, y_pred):\n",
    "        from tensorflow.python.keras import backend as K\n",
    "        return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n",
    "    model.compile(optimizer=RMSprop(),loss='mse',metrics=[soft_acc])\n",
    "    model.fit(data[train], tags[train], epochs=2000, verbose=0)\n",
    "    scores = model.evaluate(data[test], tags[test], verbose=0)\n",
    "    print(scores[1]*100)\n",
    "    cvs_scores.append(scores[1]*100)\n",
    "print(\"Accuracy:\", np.mean(cvs_scores),\"std:\", np.std(cvs_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 567 samples, validate on 183 samples\n",
      "Epoch 1/1000\n",
      "567/567 [==============================] - 1s 1ms/step - loss: 12.8269 - soft_acc: 0.3122 - val_loss: 10.4141 - val_soft_acc: 0.4098\n",
      "Epoch 2/1000\n",
      "567/567 [==============================] - 0s 644us/step - loss: 9.0257 - soft_acc: 0.3404 - val_loss: 7.4448 - val_soft_acc: 0.3880\n",
      "Epoch 3/1000\n",
      "567/567 [==============================] - 0s 653us/step - loss: 6.5777 - soft_acc: 0.3439 - val_loss: 5.3564 - val_soft_acc: 0.3989\n",
      "Epoch 4/1000\n",
      "567/567 [==============================] - 0s 627us/step - loss: 4.6956 - soft_acc: 0.3721 - val_loss: 3.7391 - val_soft_acc: 0.3880\n",
      "Epoch 5/1000\n",
      "567/567 [==============================] - 0s 637us/step - loss: 3.2329 - soft_acc: 0.3739 - val_loss: 2.7400 - val_soft_acc: 0.3333\n",
      "Epoch 6/1000\n",
      "567/567 [==============================] - 0s 642us/step - loss: 2.2974 - soft_acc: 0.4092 - val_loss: 1.8975 - val_soft_acc: 0.4372\n",
      "Epoch 7/1000\n",
      "567/567 [==============================] - 0s 635us/step - loss: 1.7667 - soft_acc: 0.4339 - val_loss: 1.5807 - val_soft_acc: 0.4153\n",
      "Epoch 8/1000\n",
      "567/567 [==============================] - 0s 630us/step - loss: 1.5281 - soft_acc: 0.4092 - val_loss: 1.4027 - val_soft_acc: 0.5410\n",
      "Epoch 9/1000\n",
      "567/567 [==============================] - 0s 634us/step - loss: 1.3313 - soft_acc: 0.4145 - val_loss: 1.2697 - val_soft_acc: 0.3333\n",
      "Epoch 10/1000\n",
      "567/567 [==============================] - 0s 632us/step - loss: 1.2170 - soft_acc: 0.4162 - val_loss: 1.3252 - val_soft_acc: 0.3388\n",
      "Epoch 11/1000\n",
      "567/567 [==============================] - 0s 638us/step - loss: 1.1390 - soft_acc: 0.4480 - val_loss: 1.1045 - val_soft_acc: 0.5355\n",
      "Epoch 12/1000\n",
      "567/567 [==============================] - 0s 630us/step - loss: 1.0641 - soft_acc: 0.4356 - val_loss: 1.0441 - val_soft_acc: 0.4809\n",
      "Epoch 13/1000\n",
      "567/567 [==============================] - 0s 636us/step - loss: 1.0276 - soft_acc: 0.4374 - val_loss: 1.0033 - val_soft_acc: 0.4262\n",
      "Epoch 14/1000\n",
      "567/567 [==============================] - 0s 637us/step - loss: 1.0106 - soft_acc: 0.4392 - val_loss: 1.1480 - val_soft_acc: 0.3388\n",
      "Epoch 15/1000\n",
      "567/567 [==============================] - 0s 634us/step - loss: 0.9770 - soft_acc: 0.4462 - val_loss: 0.9871 - val_soft_acc: 0.3388\n",
      "Epoch 16/1000\n",
      "567/567 [==============================] - 0s 636us/step - loss: 0.9467 - soft_acc: 0.4409 - val_loss: 0.9507 - val_soft_acc: 0.4372\n",
      "Epoch 17/1000\n",
      "567/567 [==============================] - 0s 633us/step - loss: 0.9219 - soft_acc: 0.4480 - val_loss: 0.9306 - val_soft_acc: 0.4098\n",
      "Epoch 18/1000\n",
      "567/567 [==============================] - 0s 634us/step - loss: 0.9159 - soft_acc: 0.4427 - val_loss: 1.0808 - val_soft_acc: 0.4754\n",
      "Epoch 19/1000\n",
      "567/567 [==============================] - 0s 626us/step - loss: 0.9077 - soft_acc: 0.4497 - val_loss: 0.9359 - val_soft_acc: 0.3388\n",
      "Epoch 20/1000\n",
      "567/567 [==============================] - 0s 630us/step - loss: 0.9180 - soft_acc: 0.4427 - val_loss: 0.8911 - val_soft_acc: 0.4098\n",
      "Epoch 21/1000\n",
      "567/567 [==============================] - 0s 681us/step - loss: 0.8881 - soft_acc: 0.4515 - val_loss: 0.9635 - val_soft_acc: 0.6066\n",
      "Epoch 22/1000\n",
      "567/567 [==============================] - 0s 680us/step - loss: 0.8885 - soft_acc: 0.4497 - val_loss: 0.9688 - val_soft_acc: 0.3333\n",
      "Epoch 23/1000\n",
      "567/567 [==============================] - 0s 699us/step - loss: 0.8608 - soft_acc: 0.4533 - val_loss: 0.8866 - val_soft_acc: 0.4536\n",
      "Epoch 24/1000\n",
      "567/567 [==============================] - 0s 681us/step - loss: 0.8465 - soft_acc: 0.4515 - val_loss: 0.9172 - val_soft_acc: 0.5519\n",
      "Epoch 25/1000\n",
      "567/567 [==============================] - 0s 647us/step - loss: 0.8571 - soft_acc: 0.4533 - val_loss: 0.8670 - val_soft_acc: 0.3607\n",
      "Epoch 26/1000\n",
      "567/567 [==============================] - 0s 674us/step - loss: 0.8487 - soft_acc: 0.4497 - val_loss: 0.8625 - val_soft_acc: 0.4262\n",
      "Epoch 27/1000\n",
      "567/567 [==============================] - 0s 669us/step - loss: 0.8461 - soft_acc: 0.4497 - val_loss: 0.8757 - val_soft_acc: 0.4809\n",
      "Epoch 28/1000\n",
      "567/567 [==============================] - 0s 637us/step - loss: 0.8270 - soft_acc: 0.4550 - val_loss: 0.8562 - val_soft_acc: 0.3497\n",
      "Epoch 29/1000\n",
      "567/567 [==============================] - 0s 669us/step - loss: 0.8342 - soft_acc: 0.4550 - val_loss: 0.8487 - val_soft_acc: 0.4044\n",
      "Epoch 30/1000\n",
      "567/567 [==============================] - 0s 643us/step - loss: 0.8314 - soft_acc: 0.4550 - val_loss: 0.8501 - val_soft_acc: 0.4317\n",
      "Epoch 31/1000\n",
      "567/567 [==============================] - 0s 627us/step - loss: 0.8142 - soft_acc: 0.4533 - val_loss: 1.0954 - val_soft_acc: 0.3333\n",
      "Epoch 32/1000\n",
      "567/567 [==============================] - 0s 635us/step - loss: 0.8354 - soft_acc: 0.4568 - val_loss: 0.8417 - val_soft_acc: 0.3661\n",
      "Epoch 33/1000\n",
      "567/567 [==============================] - 0s 630us/step - loss: 0.8142 - soft_acc: 0.4515 - val_loss: 0.8540 - val_soft_acc: 0.4754\n",
      "Epoch 34/1000\n",
      "567/567 [==============================] - 0s 669us/step - loss: 0.8042 - soft_acc: 0.4533 - val_loss: 0.8367 - val_soft_acc: 0.3607\n",
      "Epoch 35/1000\n",
      "567/567 [==============================] - 0s 641us/step - loss: 0.8157 - soft_acc: 0.4533 - val_loss: 0.9107 - val_soft_acc: 0.6230\n",
      "Epoch 36/1000\n",
      "567/567 [==============================] - 0s 633us/step - loss: 0.8066 - soft_acc: 0.4586 - val_loss: 0.8308 - val_soft_acc: 0.3607\n",
      "Epoch 37/1000\n",
      "567/567 [==============================] - 0s 625us/step - loss: 0.8154 - soft_acc: 0.4497 - val_loss: 0.8251 - val_soft_acc: 0.3989\n",
      "Epoch 38/1000\n",
      "567/567 [==============================] - 0s 626us/step - loss: 0.7925 - soft_acc: 0.4515 - val_loss: 0.8619 - val_soft_acc: 0.5246\n",
      "Epoch 39/1000\n",
      "567/567 [==============================] - 0s 634us/step - loss: 0.8120 - soft_acc: 0.4497 - val_loss: 0.9670 - val_soft_acc: 0.5082\n",
      "Epoch 40/1000\n",
      "567/567 [==============================] - 0s 632us/step - loss: 0.7987 - soft_acc: 0.4497 - val_loss: 0.8528 - val_soft_acc: 0.5137\n",
      "Epoch 41/1000\n",
      "567/567 [==============================] - 0s 627us/step - loss: 0.7951 - soft_acc: 0.4515 - val_loss: 0.8557 - val_soft_acc: 0.5355\n",
      "Epoch 42/1000\n",
      "567/567 [==============================] - 0s 629us/step - loss: 0.8017 - soft_acc: 0.4497 - val_loss: 0.8377 - val_soft_acc: 0.4973\n",
      "Epoch 43/1000\n",
      "567/567 [==============================] - 0s 630us/step - loss: 0.7853 - soft_acc: 0.4568 - val_loss: 0.9422 - val_soft_acc: 0.3333\n",
      "Epoch 44/1000\n",
      "567/567 [==============================] - 0s 632us/step - loss: 0.7817 - soft_acc: 0.4568 - val_loss: 0.8775 - val_soft_acc: 0.3333\n",
      "Epoch 45/1000\n",
      "567/567 [==============================] - 0s 627us/step - loss: 0.7915 - soft_acc: 0.4550 - val_loss: 0.8274 - val_soft_acc: 0.3388\n",
      "Epoch 46/1000\n",
      "567/567 [==============================] - 0s 636us/step - loss: 0.7840 - soft_acc: 0.4515 - val_loss: 0.8195 - val_soft_acc: 0.4590\n",
      "Epoch 47/1000\n",
      "567/567 [==============================] - 0s 643us/step - loss: 0.7745 - soft_acc: 0.4497 - val_loss: 0.8133 - val_soft_acc: 0.3770\n",
      "Epoch 48/1000\n",
      "567/567 [==============================] - 0s 644us/step - loss: 0.7936 - soft_acc: 0.4533 - val_loss: 0.8162 - val_soft_acc: 0.3497\n",
      "Epoch 49/1000\n",
      "567/567 [==============================] - 0s 649us/step - loss: 0.7743 - soft_acc: 0.4550 - val_loss: 0.8629 - val_soft_acc: 0.5574\n",
      "Epoch 50/1000\n",
      "567/567 [==============================] - 0s 645us/step - loss: 0.7719 - soft_acc: 0.4533 - val_loss: 0.8096 - val_soft_acc: 0.3880\n",
      "Epoch 51/1000\n",
      "567/567 [==============================] - 0s 632us/step - loss: 0.7650 - soft_acc: 0.4550 - val_loss: 0.8391 - val_soft_acc: 0.3388\n",
      "Epoch 52/1000\n",
      "567/567 [==============================] - 0s 627us/step - loss: 0.7697 - soft_acc: 0.4515 - val_loss: 0.8264 - val_soft_acc: 0.3388\n",
      "Epoch 53/1000\n",
      "567/567 [==============================] - 0s 630us/step - loss: 0.7726 - soft_acc: 0.4603 - val_loss: 0.8042 - val_soft_acc: 0.3880\n",
      "Epoch 54/1000\n",
      "567/567 [==============================] - 0s 635us/step - loss: 0.7657 - soft_acc: 0.4533 - val_loss: 0.8315 - val_soft_acc: 0.5355\n",
      "Epoch 55/1000\n",
      "567/567 [==============================] - 0s 627us/step - loss: 0.7714 - soft_acc: 0.4568 - val_loss: 0.8143 - val_soft_acc: 0.4809\n",
      "Epoch 56/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567/567 [==============================] - 0s 625us/step - loss: 0.7653 - soft_acc: 0.4497 - val_loss: 0.8840 - val_soft_acc: 0.3333\n",
      "Epoch 57/1000\n",
      "567/567 [==============================] - 0s 652us/step - loss: 0.7756 - soft_acc: 0.4515 - val_loss: 0.8000 - val_soft_acc: 0.3934\n",
      "Epoch 58/1000\n",
      "567/567 [==============================] - 0s 646us/step - loss: 0.7694 - soft_acc: 0.4515 - val_loss: 0.8068 - val_soft_acc: 0.4645\n",
      "Epoch 59/1000\n",
      "567/567 [==============================] - 0s 666us/step - loss: 0.7615 - soft_acc: 0.4533 - val_loss: 0.8115 - val_soft_acc: 0.3497\n",
      "Epoch 60/1000\n",
      "567/567 [==============================] - 0s 671us/step - loss: 0.7605 - soft_acc: 0.4603 - val_loss: 0.8134 - val_soft_acc: 0.3497\n",
      "Epoch 61/1000\n",
      "567/567 [==============================] - 0s 678us/step - loss: 0.7707 - soft_acc: 0.4497 - val_loss: 0.7955 - val_soft_acc: 0.4044\n",
      "Epoch 62/1000\n",
      "567/567 [==============================] - 0s 664us/step - loss: 0.7538 - soft_acc: 0.4533 - val_loss: 0.8008 - val_soft_acc: 0.3552\n",
      "Epoch 63/1000\n",
      "567/567 [==============================] - 0s 649us/step - loss: 0.7556 - soft_acc: 0.4515 - val_loss: 0.8264 - val_soft_acc: 0.5301\n",
      "Epoch 64/1000\n",
      "567/567 [==============================] - 0s 656us/step - loss: 0.7577 - soft_acc: 0.4533 - val_loss: 0.8017 - val_soft_acc: 0.4590\n",
      "Epoch 65/1000\n",
      "567/567 [==============================] - 0s 719us/step - loss: 0.7570 - soft_acc: 0.4586 - val_loss: 0.8273 - val_soft_acc: 0.3388\n",
      "Epoch 66/1000\n",
      "567/567 [==============================] - 0s 669us/step - loss: 0.7569 - soft_acc: 0.4550 - val_loss: 0.8030 - val_soft_acc: 0.3497\n",
      "Epoch 67/1000\n",
      "567/567 [==============================] - 0s 657us/step - loss: 0.7626 - soft_acc: 0.4533 - val_loss: 0.7969 - val_soft_acc: 0.3552\n",
      "Epoch 68/1000\n",
      "567/567 [==============================] - 0s 647us/step - loss: 0.7478 - soft_acc: 0.4515 - val_loss: 0.7972 - val_soft_acc: 0.3716\n",
      "Epoch 69/1000\n",
      "567/567 [==============================] - 0s 637us/step - loss: 0.7434 - soft_acc: 0.4515 - val_loss: 0.8163 - val_soft_acc: 0.3497\n",
      "Epoch 70/1000\n",
      "567/567 [==============================] - 0s 643us/step - loss: 0.7556 - soft_acc: 0.4603 - val_loss: 0.7967 - val_soft_acc: 0.3497\n",
      "Epoch 71/1000\n",
      "567/567 [==============================] - 0s 630us/step - loss: 0.7512 - soft_acc: 0.4497 - val_loss: 0.7972 - val_soft_acc: 0.3497\n",
      "Epoch 72/1000\n",
      "567/567 [==============================] - 0s 612us/step - loss: 0.7479 - soft_acc: 0.4550 - val_loss: 0.8324 - val_soft_acc: 0.3388\n",
      "Epoch 73/1000\n",
      "567/567 [==============================] - 0s 597us/step - loss: 0.7541 - soft_acc: 0.4550 - val_loss: 0.7933 - val_soft_acc: 0.4699\n",
      "Epoch 74/1000\n",
      "567/567 [==============================] - 0s 685us/step - loss: 0.7390 - soft_acc: 0.4550 - val_loss: 0.8140 - val_soft_acc: 0.5410\n",
      "Epoch 75/1000\n",
      "567/567 [==============================] - 0s 652us/step - loss: 0.7589 - soft_acc: 0.4621 - val_loss: 0.8050 - val_soft_acc: 0.5082\n",
      "Epoch 76/1000\n",
      "567/567 [==============================] - 0s 661us/step - loss: 0.7406 - soft_acc: 0.4568 - val_loss: 0.7888 - val_soft_acc: 0.4699\n",
      "Epoch 77/1000\n",
      "567/567 [==============================] - 0s 648us/step - loss: 0.7471 - soft_acc: 0.4480 - val_loss: 0.7872 - val_soft_acc: 0.3607\n",
      "Epoch 78/1000\n",
      "567/567 [==============================] - 0s 609us/step - loss: 0.7419 - soft_acc: 0.4515 - val_loss: 0.7902 - val_soft_acc: 0.3716\n",
      "Epoch 79/1000\n",
      "567/567 [==============================] - 0s 612us/step - loss: 0.7433 - soft_acc: 0.4515 - val_loss: 0.7857 - val_soft_acc: 0.3607\n",
      "Epoch 80/1000\n",
      "567/567 [==============================] - 0s 605us/step - loss: 0.7427 - soft_acc: 0.4515 - val_loss: 0.8642 - val_soft_acc: 0.5902\n",
      "Epoch 81/1000\n",
      "567/567 [==============================] - 0s 604us/step - loss: 0.7405 - soft_acc: 0.4674 - val_loss: 0.7808 - val_soft_acc: 0.4536\n",
      "Epoch 82/1000\n",
      "567/567 [==============================] - 0s 615us/step - loss: 0.7368 - soft_acc: 0.4515 - val_loss: 0.7772 - val_soft_acc: 0.4153\n",
      "Epoch 83/1000\n",
      "567/567 [==============================] - 0s 609us/step - loss: 0.7421 - soft_acc: 0.4568 - val_loss: 0.7948 - val_soft_acc: 0.5082\n",
      "Epoch 84/1000\n",
      "567/567 [==============================] - 0s 615us/step - loss: 0.7435 - soft_acc: 0.4497 - val_loss: 0.7757 - val_soft_acc: 0.3989\n",
      "Epoch 85/1000\n",
      "567/567 [==============================] - 0s 613us/step - loss: 0.7399 - soft_acc: 0.4638 - val_loss: 0.7905 - val_soft_acc: 0.3552\n",
      "Epoch 86/1000\n",
      "567/567 [==============================] - 0s 636us/step - loss: 0.7319 - soft_acc: 0.4515 - val_loss: 0.8233 - val_soft_acc: 0.3388\n",
      "Epoch 87/1000\n",
      "567/567 [==============================] - 0s 620us/step - loss: 0.7305 - soft_acc: 0.4586 - val_loss: 0.7797 - val_soft_acc: 0.4699\n",
      "Epoch 88/1000\n",
      "450/567 [======================>.......] - ETA: 0s - loss: 0.7464 - soft_acc: 0.4422"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_soft_acc',patience=100, mode='max')\n",
    "cp = ModelCheckpoint('model-{epoch:03d}-{soft_acc:03f}-{val_soft_acc:03f}.h5', verbose=1, monitor='val_soft_acc',save_best_only=True, mode='max')  \n",
    "model.compile(optimizer=RMSprop(),loss='mse',metrics=[soft_acc])\n",
    "history = model.fit(data, tags,epochs=1000,  batch_size=50,validation_data=(old_data, old_tags),callbacks=[])\n",
    "# model.save_weights('pre_trained_glove_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "soft_acc = history.history['soft_acc']\n",
    "soft_val_acc = history.history['val_soft_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(soft_acc) + 1)\n",
    "plt.figure(figsize=(30,5))\n",
    "plt.plot(epochs, soft_acc, 'bo', label='Soft Training acc')\n",
    "plt.plot(epochs, soft_val_acc, 'b', label='Soft Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure(figsize=(30,5))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
